{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6ee3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "#import packages\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#local session \n",
    "spark = (SparkSession.builder.master(\"local[2]\").appName(\"logistic-regression\").getOrCreate())\n",
    "\n",
    "# Set the data path\n",
    "rescue_path_parquet = '/training/rescue_clean.parquet'\n",
    "\n",
    "# Read in the data\n",
    "rescue = spark.read.parquet(rescue_path_parquet)\n",
    "\n",
    "rescue.limit(5).toPandas()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7343b392",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create is_cat column to contain target variable and select relevant predictors\n",
    "rescue_cat = rescue.withColumn('is_cat', \n",
    "                               F.when(F.col('animal_group')==\"Cat\", 1)\n",
    "                               .otherwise(0)).select(\"typeofincident\", \n",
    "                              \"engine_count\", \n",
    "                              \"job_hours\", \n",
    "                              \"hourly_cost\", \n",
    "                              \"total_cost\", \n",
    "                              \"originofcall\", \n",
    "                              \"propertycategory\",\n",
    "                              \"specialservicetypecategory\",\n",
    "                              \"incident_duration\",\n",
    "                              \"is_cat\")\n",
    "\n",
    "# Check created column\n",
    "rescue_cat.limit(20).toPandas()\n",
    "\n",
    "# Check data types\n",
    "rescue_cat.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c30f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert engine_count, job_hours, hourly_cost, total_cost and incident_duration columns to numeric\n",
    "rescue_cat = (\n",
    "  rescue_cat.withColumn(\"engine_count\", F.col(\"engine_count\").cast(\"double\"))\n",
    "            .withColumn(\"job_hours\", F.col(\"job_hours\").cast(\"double\"))\n",
    "            .withColumn(\"hourly_cost\", F.col(\"hourly_cost\").cast(\"double\"))\n",
    "            .withColumn(\"total_cost\", F.col(\"total_cost\").cast(\"double\"))\n",
    "            .withColumn(\"incident_duration\", F.col(\"incident_duration\").cast(\"double\")))\n",
    "\n",
    "\n",
    "# Check data types are now correct\n",
    "rescue_cat.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f334f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the count of missing values for each column\n",
    "missing_summary = (\n",
    "    rescue_cat\n",
    "    .select([F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in rescue_cat.columns])\n",
    ")\n",
    "\n",
    "# Show the summary\n",
    "missing_summary.show(vertical = True)\n",
    "\n",
    "# We can see that these are all on the same rows by filtering for NAs in one of the columns:\n",
    "rescue_cat.filter(rescue_cat.total_cost.isNull()).limit(38).toPandas()\n",
    "\n",
    "# For simplicity, we will just filter out these rows:\n",
    "rescue_cat = rescue_cat.na.drop()\n",
    "\n",
    "# Double check we have no nulls left:\n",
    "missing_summary = (\n",
    "    rescue_cat\n",
    "    .select([F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in rescue_cat.columns])\n",
    "    .show(vertical = True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53e545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing the required libraries - replace OneHotEncoderEstimator with OneHotEncoder if using Spark >= 3.0\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoderEstimator\n",
    "\n",
    "## First we call the StringIndexer separately for each categorical variable\n",
    "\n",
    "# Indexing the specialservicetypecategory column\n",
    "serviceIdx = StringIndexer(inputCol='specialservicetypecategory',\n",
    "                               outputCol='serviceIndex')\n",
    "\n",
    "# Indexing the originofcallcolumn\n",
    "callIdx = StringIndexer(inputCol='originofcall',\n",
    "                               outputCol='callIndex')\n",
    "\n",
    "# Indexing the propertycategory column\n",
    "propertyIdx = StringIndexer(inputCol='propertycategory',\n",
    "                               outputCol='propertyIndex')\n",
    "                               \n",
    "# Apply indexing to each column one by one\n",
    "\n",
    "rescue_cat_indexed = serviceIdx.fit(rescue_cat).transform(rescue_cat)\n",
    "rescue_cat_indexed = callIdx.fit(rescue_cat_indexed).transform(rescue_cat_indexed)\n",
    "rescue_cat_indexed = propertyIdx.fit(rescue_cat_indexed).transform(rescue_cat_indexed)\n",
    "\n",
    "# Check that this has worked correctly\n",
    "rescue_cat_indexed.select('is_cat', 'specialservicetypecategory', 'originofcall',\n",
    "                          'propertycategory', 'serviceIndex', 'callIndex', \n",
    "                          'propertyIndex').show(10, truncate = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6af9bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply OneHotEncoderEstimator to each categorical column simultaneously\n",
    "# Replace OneHotEncoderEstimator with OneHotEncoder if using Spark >= 3.0\n",
    "encoder = OneHotEncoderEstimator(inputCols = ['serviceIndex', 'callIndex', 'propertyIndex'], \n",
    "                                 outputCols = ['serviceVec', 'callVec', 'propertyVec'])\n",
    "\n",
    "rescue_cat_ohe = encoder.fit(rescue_cat_indexed).transform(rescue_cat_indexed)\n",
    "\n",
    "# Check that this has worked correctly \n",
    "rescue_cat_ohe.select('is_cat', 'specialservicetypecategory', 'originofcall',\n",
    "                          'propertycategory', 'serviceVec', 'callVec', \n",
    "                          'propertyVec').show(10, truncate = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbd6ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Call 'VectorAssembler' to vectorise all predictor columns in dataset\n",
    "assembler = VectorAssembler(inputCols=['engine_count', 'job_hours', 'hourly_cost',\n",
    "                                       'callVec', 'propertyVec', 'serviceVec'],\n",
    "                            outputCol = \"features\")\n",
    " \n",
    "# Apply vectorisation                                      \n",
    "rescue_cat_vectorised = assembler.transform(rescue_cat_ohe)\n",
    "\n",
    "# Rename \"is_cat\" target variable column to \"label\" ready to pass to the regression model\n",
    "rescue_cat_final = rescue_cat_vectorised.withColumnRenamed(\"is_cat\", \"label\").select(\"label\", \"features\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a55531",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import GeneralizedLinearRegression\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "\n",
    "# Define model - specify family and link as shown for logistic regression\n",
    "glr = GeneralizedLinearRegression(family=\"binomial\", link=\"logit\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5533462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run model\n",
    "model = glr.fit(rescue_cat_final)\n",
    "\n",
    "# Get model results\n",
    "model_output = model.transform(rescue_cat_final)\n",
    "model_output.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2202b587",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get model summary\n",
    "summary = model.summary\n",
    "\n",
    "# Show summary\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd15c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get model output\n",
    "model_output = model.transform(rescue_cat_final)\n",
    "\n",
    "# Get feature names from the model output metadata\n",
    "# Numeric and binary (categorical) metadata are accessed separately\n",
    "numeric_metadata = model_output.select(\"features\").schema[0].metadata.get('ml_attr').get('attrs').get('numeric')\n",
    "binary_metadata = model_output.select(\"features\").schema[0].metadata.get('ml_attr').get('attrs').get('binary')\n",
    "\n",
    "# Merge the numeric and binary metadata lists to get all the feature names\n",
    "merge_list = numeric_metadata + binary_metadata\n",
    "\n",
    "# Convert the feature name list to a Pandas dataframe\n",
    "full_summary = pd.DataFrame(merge_list)\n",
    "\n",
    "# Get the regression coefficients from the model\n",
    "full_summary['coefficients'] = model.coefficients\n",
    "\n",
    "# The intercept coefficient needs to be added in separately since it is not part of the features metadata\n",
    "# Define a new row for the intercept coefficient and get value from model\n",
    "intercept = pd.DataFrame({'name':'intercept', 'coefficients':model.intercept}, index = [0])\n",
    "\n",
    "# Add new row to the top of the full_summary dataframe\n",
    "full_summary = pd.concat([intercept,full_summary.loc[:]]).reset_index(drop=True)\n",
    "\n",
    "# Add standard errors, t-values and p-values from summary into the full_summary dataframe:\n",
    "full_summary['std_error'] = summary.coefficientStandardErrors\n",
    "full_summary['tvalues'] = summary.tValues\n",
    "full_summary['pvalues'] = summary.pValues\n",
    "\n",
    "# Manually calculate upper and lower confidence bounds and add into dataframe\n",
    "full_summary['upper_ci'] = full_summary['coefficients'] + (1.96*full_summary['std_error'])\n",
    "full_summary['lower_ci'] = full_summary['coefficients'] - (1.96*full_summary['std_error'])\n",
    "\n",
    "# View final model summary\n",
    "full_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddefe56",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# Add the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79beb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert typeofincident column into numeric value\n",
    "rescue_cat_singular = rescue_cat_ohe.withColumn('typeofincident', F.when(F.col('typeofincident')==\"Special Service\", 1)\n",
    "                               .otherwise(0))\n",
    "\n",
    "# Setup the vectorassembler to include this variable in the features column\n",
    "assembler = VectorAssembler(inputCols=['typeofincident', 'engine_count', 'job_hours', 'hourly_cost', \n",
    "                                       'callVec', 'propertyVec', 'serviceVec'], \n",
    "                           outputCol = \"features\")\n",
    "\n",
    "rescue_cat_vectorised_sing = assembler.transform(rescue_cat_singular)\n",
    "\n",
    "\n",
    "rescue_cat_final_sing = rescue_cat_vectorised_sing.withColumnRenamed(\"is_cat\", \"label\").select(\"label\", \"features\")\n",
    "\n",
    "# Run the model\n",
    "model_sing = glr.fit(rescue_cat_final_sing)\n",
    "\n",
    "# Return model summary (will give an error)\n",
    "summary_sing = model_sing.summary\n",
    "\n",
    "summary_sing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2998d13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rescue_cat.groupBy(\"specialservicetypecategory\").count().orderBy(\"count\").show(truncate = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59dc422",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add \"000_\" prefix to selected reference categories\n",
    "\n",
    "rescue_cat_reindex = (rescue_cat\n",
    "                      .withColumn('specialservicetypecategory', \n",
    "                                       F.when(F.col('specialservicetypecategory')==\"Other Animal Assistance\", \"000_Other Animal Assistance\")\n",
    "                                       .otherwise(F.col('specialservicetypecategory')))\n",
    "                      .withColumn('originofcall', \n",
    "                                       F.when(F.col('originofcall') == \"Person (mobile)\", \"000_Person (mobile)\")\n",
    "                                       .otherwise(F.col('originofcall')))\n",
    "                      .withColumn('propertycategory', \n",
    "                                       F.when(F.col('propertycategory') == \"Dwelling\", \"000_Dwelling\")\n",
    "                                       .otherwise(F.col('propertycategory'))))\n",
    "\n",
    "# Check prefix additions \n",
    "rescue_cat_reindex.select('specialservicetypecategory', 'originofcall', 'propertycategory').show(20)\n",
    "\n",
    "# Use stringOrderType argument of StringIndexer\n",
    "\n",
    "# Re-indexing the specialservicetypecategory column\n",
    "serviceIdx = StringIndexer(inputCol='specialservicetypecategory',\n",
    "                               outputCol='serviceIndex', \n",
    "                               stringOrderType = \"alphabetDesc\")\n",
    "\n",
    "# Indexing the originofcallcolumn\n",
    "callIdx = StringIndexer(inputCol='originofcall',\n",
    "                               outputCol='callIndex',\n",
    "                               stringOrderType = \"alphabetDesc\")\n",
    "\n",
    "# Indexing the propertycategory column\n",
    "propertyIdx = StringIndexer(inputCol='propertycategory',\n",
    "                               outputCol='propertyIndex', \n",
    "                               stringOrderType = \"alphabetDesc\")\n",
    "\n",
    "# Call indexing for each column one by one\n",
    "\n",
    "rescue_cat_indexed = serviceIdx.fit(rescue_cat_reindex).transform(rescue_cat_reindex)\n",
    "rescue_cat_indexed = callIdx.fit(rescue_cat_indexed).transform(rescue_cat_indexed)\n",
    "rescue_cat_indexed = propertyIdx.fit(rescue_cat_indexed).transform(rescue_cat_indexed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e65885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Encode re-indexed columns\n",
    "encoder = OneHotEncoderEstimator(inputCols = ['serviceIndex', 'callIndex', 'propertyIndex'], \n",
    "                                 outputCols = ['serviceVec', 'callVec', 'propertyVec'])\n",
    "\n",
    "rescue_cat_ohe = encoder.fit(rescue_cat_indexed).transform(rescue_cat_indexed)\n",
    "\n",
    "\n",
    "# Vectorize all our predictors into a new column called \"features\" \n",
    "\n",
    "assembler = VectorAssembler(inputCols=['engine_count', 'job_hours', 'hourly_cost', \n",
    "                                       'callVec', 'propertyVec', 'serviceVec'], \n",
    "                           outputCol = \"features\")\n",
    "\n",
    "rescue_cat_vectorised = assembler.transform(rescue_cat_ohe)\n",
    "\n",
    "# Rename target variable \"is_cat\" to \"label\" ready to run regression model\n",
    "rescue_cat_final = rescue_cat_vectorised.withColumnRenamed(\"is_cat\", \"label\").select(\"label\", \"features\")\n",
    "\n",
    "# Run the model again\n",
    "model = glr.fit(rescue_cat_final)\n",
    "\n",
    "# Show summary\n",
    "model.summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c498696f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "# Select feature column vector\n",
    "features_vector = rescue_cat_final.select(\"features\")\n",
    "\n",
    "# Generate correlation matrix\n",
    "matrix = Correlation.corr(features_vector, \"features\").collect()[0][0]\n",
    "\n",
    "# Convert matrix into a useful format\n",
    "corr_matrix = matrix.toArray().tolist() \n",
    "\n",
    "# Get list of features to assign to matrix columns and indices\n",
    "features = pd.DataFrame(merge_list)['name'].values.tolist()\n",
    "\n",
    "# Final correlation matrix\n",
    "corr_matrix_df = pd.DataFrame(data=corr_matrix, columns = features, index = features) \n",
    "\n",
    "corr_matrix_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87d9fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rescue_cat.select(\"job_hours\", \"hourly_cost\", \"total_cost\").orderBy(\"job_hours\", ascending=False).limit(30).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95fc92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Rename \"is_cat\" to \"label\" before setting up pipeline stages\n",
    "rescue_cat_reindex = rescue_cat_reindex.withColumnRenamed(\"is_cat\", \"label\")\n",
    "\n",
    "# 1. Indexing the specialservicetypecategory column\n",
    "serviceIdx = StringIndexer(inputCol='specialservicetypecategory',\n",
    "                               outputCol='serviceIndex', \n",
    "                               stringOrderType = \"alphabetDesc\")\n",
    "\n",
    "# 2. Indexing the originofcall column\n",
    "callIdx = StringIndexer(inputCol='originofcall',\n",
    "                               outputCol='callIndex',\n",
    "                               stringOrderType = \"alphabetDesc\")\n",
    "\n",
    "# 3. Indexing the propertycategory column\n",
    "propertyIdx = StringIndexer(inputCol='propertycategory',\n",
    "                               outputCol='propertyIndex', \n",
    "                               stringOrderType = \"alphabetDesc\")\n",
    "\n",
    "# 4. One-hot encoding\n",
    "encoder = OneHotEncoderEstimator(inputCols = ['serviceIndex', 'callIndex', 'propertyIndex'], \n",
    "                                 outputCols = ['serviceVec', 'callVec', 'propertyVec'])\n",
    "\n",
    "# 5. Vector assembler\n",
    "assembler = VectorAssembler(inputCols=['engine_count', 'hourly_cost', \n",
    "                                       'callVec', 'propertyVec', 'serviceVec'], \n",
    "                           outputCol = \"features\")\n",
    "\n",
    "# 6. Regression model\n",
    "glr = GeneralizedLinearRegression(family=\"binomial\", link=\"logit\")\n",
    "\n",
    "# Creating the pipeline\n",
    "pipe = Pipeline(stages=[serviceIdx, callIdx, propertyIdx,\n",
    "                        encoder, assembler, glr])\n",
    "                        \n",
    "# View the pipeline stages\n",
    "pipe.getStages()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cb872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fit_model = pipe.fit(rescue_cat_reindex)\n",
    "\n",
    "# Save model results\n",
    "results = fit_model.transform(rescue_cat_reindex)\n",
    "  \n",
    "# Showing the results\n",
    "results.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3322827d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get coefficients summary table\n",
    "summary = fit_model.stages[-1].summary\n",
    "\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10395eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save pipeline\n",
    "\n",
    "pipe.write().overwrite().save(\"rescue_pipeline\")\n",
    "\n",
    "# Save the pipeline model\n",
    "\n",
    "fit_model.write().overwrite().save(\"rescue_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a036b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load saved pipeline\n",
    "reloaded_pipeline = Pipeline.load(\"rescue_pipeline\")\n",
    "\n",
    "# Re-fit to a subset of rescue data as an example of how pipelines can be re-used\n",
    "new_model = reloaded_pipeline.fit(rescue_cat_reindex.sample(withReplacement=None,\n",
    "                      fraction=0.1, seed = 99))\n",
    "                      \n",
    "# View new model summary\n",
    "new_model.stages[-1].summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7a4c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Close the spark session\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
