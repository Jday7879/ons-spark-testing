{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Cramér's V from a Spark DataFrame\n",
    "\n",
    "### What is Cramér's V?\n",
    "Cramér's V is a statistical measure of an association between two nominal variables, giving a value between 0 and 1 inclusive. Here 0 would indicate no association and 1 indicates a strong association between the two variables. It is based on Pearson's chi-square statistics.\n",
    "\n",
    "We calculate Cramér's V as follows:\n",
    "\n",
    "$$ \\text{Cramer's V} = \\sqrt{\\dfrac{\\dfrac{\\chi^2}{n}}{\\min (c-1,r-1)}}, $$ \n",
    "where:\n",
    "- $\\chi^2$ is the Chi-squared statistic,\n",
    "- $n$ is the number of samples,\n",
    "- $r$ is the number of rows,\n",
    "- $c$ is the number of columns.\n",
    "\n",
    "In some literature you may see the Phi coefficient used ($\\phi$), where $\\phi^2 = \\chi^2/n$.\n",
    "\n",
    "### Cramér's V in Spark:\n",
    "Although there is not an in built method for calculating this statistic in base python, is it reasonably straightforward using `numpy` and `scipy.stats` packages. An example of this can be found [online here](https://www.statology.org/cramers-v-in-python/).\n",
    "A similar example for R is linked [here](https://www.statology.org/cramers-v-in-r/).\n",
    "\n",
    "Due to Pyspark and SparklyR's differences to classical python and R, we need to consider how we can calculate Cramér's V when using Spark DataFrames.\n",
    "First we will import the needed packages and start a spark session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import scipy.stats as stats\n",
    "\n",
    "spark = (SparkSession.builder.master(\"local[2]\")\n",
    "         .appName(\"cramer-v\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we will create some dummy data using the `F.rand()` spark function which will generate random numbers. For repeatable results we will set a seed.\n",
    "The `F.ceil()` function will round the number up to the nearest integer. \n",
    "After creating the dummy data, we will show the first 5 rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "|  0|   -6|\n",
      "|  1|   -5|\n",
      "|  2|   -8|\n",
      "|  3|   -2|\n",
      "|  4|   -6|\n",
      "+---+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setting random number seed\n",
    "seed_no = 42\n",
    "\n",
    "# Creating spark dataframe\n",
    "df = spark.range(100)\n",
    "df = df.union(df)\n",
    "df = df.withColumn(\"value\", F.ceil(F.rand(seed_no) * -10))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `.crosstab()` function, we can calculate a pair-wise frequency table of the `id` and `value` columns (a.k.a. contingency table). We will generate this table and convert it to a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_value</th>\n",
       "      <th>-1</th>\n",
       "      <th>-2</th>\n",
       "      <th>-3</th>\n",
       "      <th>-4</th>\n",
       "      <th>-5</th>\n",
       "      <th>-6</th>\n",
       "      <th>-7</th>\n",
       "      <th>-8</th>\n",
       "      <th>-9</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id_value  -1  -2  -3  -4  -5  -6  -7  -8  -9  0\n",
       "0        7   0   0   0   0   0   0   0   1   0  1\n",
       "1       51   0   1   0   0   0   1   0   0   0  0\n",
       "2       15   0   0   0   1   0   0   1   0   0  0\n",
       "3       54   0   0   1   0   0   1   0   0   0  0\n",
       "4       11   0   0   0   0   0   1   0   0   1  0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_spark = df.crosstab('id','value')\n",
    "freq_pandas = freq_spark.toPandas()\n",
    "freq_pandas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have brought our pair-wise frequency table into our local environment, we can now utilise the `scipy.stats.chi2_contingency()` function. From the [documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html), we can see that this takes an `array_like` input, as such we should convert our table into a Numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_numpy = np.array(freq_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example python code for this has been adapted from the article linked previously. We have opted to restructured this into a function, so it can be ran a few times. After defining the function, we will attempt to run this on our Numpy array `freq_numpy`. We have implemented a `try`-`except` statement to capture the errors and not stop the full script from running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<' not supported between instances of 'str' and 'int'\n"
     ]
    }
   ],
   "source": [
    "def get_cramer_v(freq_numpy):\n",
    "    #Chi-squared test statistic, sample size, and minimum of rows and columns\n",
    "    X2 = stats.chi2_contingency(freq_numpy, correction=False)[0]\n",
    "    n = np.sum(freq_numpy)\n",
    "    minDim = min(freq_numpy.shape)-1\n",
    "\n",
    "    #calculate Cramer's V \n",
    "    V = np.sqrt((X2/n) / minDim)\n",
    "    return V\n",
    "\n",
    "try:\n",
    "    get_cramer_v(freq_numpy)\n",
    "except TypeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `TypeError` says we have strings where we need integers. We should return to our spark pair-wise frequency table to double check the data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_value: string (nullable = false)\n",
      " |-- -1: long (nullable = false)\n",
      " |-- -2: long (nullable = false)\n",
      " |-- -3: long (nullable = false)\n",
      " |-- -4: long (nullable = false)\n",
      " |-- -5: long (nullable = false)\n",
      " |-- -6: long (nullable = false)\n",
      " |-- -7: long (nullable = false)\n",
      " |-- -8: long (nullable = false)\n",
      " |-- -9: long (nullable = false)\n",
      " |-- 0: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "freq_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the `id_value` column is a type string. So the first variable passed to `.crosstab()` comes out as string. Let's fix that using the `.cast` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "freq_spark = freq_spark.withColumn(\"id_value\", F.col(\"id_value\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_value: integer (nullable = true)\n",
      " |-- -1: long (nullable = false)\n",
      " |-- -2: long (nullable = false)\n",
      " |-- -3: long (nullable = false)\n",
      " |-- -4: long (nullable = false)\n",
      " |-- -5: long (nullable = false)\n",
      " |-- -6: long (nullable = false)\n",
      " |-- -7: long (nullable = false)\n",
      " |-- -8: long (nullable = false)\n",
      " |-- -9: long (nullable = false)\n",
      " |-- 0: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "freq_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `id_value` is `int`. Carry on with rest of processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1935164295375403\n"
     ]
    }
   ],
   "source": [
    "freq_pandas = freq_spark.toPandas()\n",
    "freq_numpy = np.array(freq_pandas)\n",
    "print(get_cramer_v(freq_numpy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we get a result.\n",
    "\n",
    "Also note that all frequencies must be positive. If there is a negative value in freq_numpy, we will get an error. \n",
    "\n",
    "To demonstrate, let's plant some negative values in `freq_numpy`, then calculate the statistic again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All values in `observed` must be nonnegative.\n"
     ]
    }
   ],
   "source": [
    "freq_numpy[0][1] = -1\n",
    "freq_numpy[10, 4] = -5\n",
    "\n",
    "try:\n",
    "    get_cramer_v(freq_numpy)\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `ValueError` tells us we have negative values. How many and where are the negative values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(freq_numpy[freq_numpy < 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two negative values. Where are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 10] [1 4]\n"
     ]
    }
   ],
   "source": [
    "rows, cols = np.where(freq_numpy < 0)\n",
    "print(rows, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They're in locations [0,1] and [10,4]. Let's see what the values are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -5])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_numpy[rows,cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further resources\n",
    "\n",
    "Spark at the ONS Articles:\n",
    "\n",
    "\n",
    "PySpark Documentation:\n",
    "- [`.rand()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.rand.html)\n",
    "- [`.ceil()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.ceil.html)\n",
    "- [`.crosstab()`](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrame.crosstab.html)\n",
    "\n",
    "Python Documentation:\n",
    "- [`subprocess`](https://docs.python.org/3/library/subprocess.html) \n",
    "\n",
    "sparklyr and tidyverse Documentation:\n",
    "- [`sdf_coalesce()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_coalesce.html)\n",
    "- [`sdf_repartition()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_repartition.html)\n",
    "- [`sdf_num_partitions()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_num_partitions.html)\n",
    "- [`spark_apply()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/spark_apply.html)\n",
    "\n",
    "Spark Documentation:\n",
    "- [Spark Configuration](https://spark.apache.org/docs/latest/configuration.html):\n",
    "    - [Execution Behaviour](https://spark.apache.org/docs/latest/configuration.html#execution-behavior)\n",
    "    - [Runtime SQL Configuration](https://spark.apache.org/docs/latest/configuration.html#runtime-sql-configuration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
