{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Cramér's V from a Spark DataFrame\n",
    "\n",
    "### What is Cramér's V?\n",
    "Cramér's V is a statistical measure of an association between two nominal variables, giving a value between 0 and 1 inclusive. Here 0 would indicate no association and 1 indicates a strong association between the two variables. It is based on Pearson's chi-square statistics.\n",
    "\n",
    "We calculate Cramér's V as follows:\n",
    "\n",
    "$$ \\text{Cramer's V} = \\sqrt{\\dfrac{\\dfrac{\\chi^2}{n}}{\\min (c-1,r-1)}}, $$ \n",
    "where:\n",
    "- $\\chi^2$ is the Chi-squared statistic,\n",
    "- $n$ is the number of samples,\n",
    "- $r$ is the number of rows,\n",
    "- $c$ is the number of columns.\n",
    "\n",
    "In some literature you may see the Phi coefficient used ($\\phi$), where $\\phi^2 = \\chi^2/n$.\n",
    "\n",
    "### Cramér's V in Spark:\n",
    "Although there is not an in built method for calculating this statistic in base python, is it reasonably straightforward using `numpy` and `scipy.stats` packages. An example of this can be found [online here](https://www.statology.org/cramers-v-in-python/).\n",
    "A similar example for R is linked [here](https://www.statology.org/cramers-v-in-r/).\n",
    "\n",
    "Due to Pyspark and SparklyR's differences to classical python and R, we need to consider how we can calculate Cramér's V when using Spark DataFrames.\n",
    "First we will import the needed packages and start a spark session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import scipy.stats as stats\n",
    "\n",
    "spark = (SparkSession.builder.master(\"local[2]\")\n",
    "         .appName(\"cramer-v\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```R\n",
    "library(sparklyr)\n",
    "library(dplyr)\n",
    "\n",
    "default_config <- sparklyr::spark_config()\n",
    "\n",
    "sc <- sparklyr::spark_connect(\n",
    "    master = \"local[2]\",\n",
    "    app_name = \"cramer-v\",\n",
    "    config = default_config)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we will create some dummy data using the `F.rand()` spark function which will generate random numbers. For repeatable results we will set a seed.\n",
    "The `F.ceil()` function will round the number up to the nearest integer. \n",
    "After creating the dummy data, we will show the first 5 rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "|  0|    7|\n",
      "|  1|    6|\n",
      "|  2|    9|\n",
      "|  3|    3|\n",
      "|  4|    7|\n",
      "|  5|    6|\n",
      "|  6|   10|\n",
      "|  7|    1|\n",
      "|  8|   10|\n",
      "|  9|    8|\n",
      "| 10|    5|\n",
      "| 11|    7|\n",
      "| 12|    4|\n",
      "| 13|    9|\n",
      "| 14|    8|\n",
      "| 15|    8|\n",
      "| 16|    1|\n",
      "| 17|    7|\n",
      "| 18|    1|\n",
      "| 19|    9|\n",
      "| 20|    5|\n",
      "| 21|    8|\n",
      "| 22|    6|\n",
      "| 23|    9|\n",
      "| 24|    8|\n",
      "| 25|    4|\n",
      "| 26|    2|\n",
      "| 27|    3|\n",
      "| 28|    8|\n",
      "| 29|    9|\n",
      "| 30|    8|\n",
      "| 31|    3|\n",
      "| 32|    8|\n",
      "| 33|    6|\n",
      "| 34|    8|\n",
      "| 35|    9|\n",
      "| 36|    2|\n",
      "| 37|    5|\n",
      "| 38|    2|\n",
      "| 39|    8|\n",
      "| 40|    3|\n",
      "| 41|    5|\n",
      "| 42|    8|\n",
      "| 43|    8|\n",
      "| 44|    6|\n",
      "| 45|    9|\n",
      "| 46|    9|\n",
      "| 47|   10|\n",
      "| 48|    2|\n",
      "| 49|    9|\n",
      "| 50|    9|\n",
      "| 51|    7|\n",
      "| 52|    3|\n",
      "| 53|    3|\n",
      "| 54|    7|\n",
      "| 55|    9|\n",
      "| 56|    9|\n",
      "| 57|    8|\n",
      "| 58|    4|\n",
      "| 59|    1|\n",
      "| 60|    7|\n",
      "| 61|    7|\n",
      "| 62|    7|\n",
      "| 63|    7|\n",
      "| 64|   10|\n",
      "| 65|    2|\n",
      "| 66|    8|\n",
      "| 67|    1|\n",
      "| 68|    9|\n",
      "| 69|    1|\n",
      "| 70|    8|\n",
      "| 71|   10|\n",
      "| 72|    7|\n",
      "| 73|    5|\n",
      "| 74|    2|\n",
      "| 75|    8|\n",
      "| 76|    1|\n",
      "| 77|    6|\n",
      "| 78|    8|\n",
      "| 79|    5|\n",
      "| 80|    7|\n",
      "| 81|    1|\n",
      "| 82|    9|\n",
      "| 83|    1|\n",
      "| 84|    1|\n",
      "| 85|    9|\n",
      "| 86|    2|\n",
      "| 87|    9|\n",
      "| 88|    3|\n",
      "| 89|    8|\n",
      "| 90|    3|\n",
      "| 91|    6|\n",
      "| 92|    3|\n",
      "| 93|    3|\n",
      "| 94|    5|\n",
      "| 95|    9|\n",
      "| 96|    7|\n",
      "| 97|    3|\n",
      "| 98|    9|\n",
      "| 99|    1|\n",
      "|  0|    8|\n",
      "|  1|    9|\n",
      "|  2|    6|\n",
      "|  3|    8|\n",
      "|  4|    8|\n",
      "|  5|    3|\n",
      "|  6|    3|\n",
      "|  7|    9|\n",
      "|  8|    3|\n",
      "|  9|   10|\n",
      "| 10|   10|\n",
      "| 11|   10|\n",
      "| 12|    6|\n",
      "| 13|    5|\n",
      "| 14|    1|\n",
      "| 15|    5|\n",
      "| 16|    1|\n",
      "| 17|    8|\n",
      "| 18|    7|\n",
      "| 19|    1|\n",
      "| 20|    7|\n",
      "| 21|    7|\n",
      "| 22|    7|\n",
      "| 23|    4|\n",
      "| 24|    6|\n",
      "| 25|    3|\n",
      "| 26|    5|\n",
      "| 27|    9|\n",
      "| 28|   10|\n",
      "| 29|    2|\n",
      "| 30|    3|\n",
      "| 31|    4|\n",
      "| 32|    7|\n",
      "| 33|   10|\n",
      "| 34|   10|\n",
      "| 35|    1|\n",
      "| 36|    4|\n",
      "| 37|    4|\n",
      "| 38|    3|\n",
      "| 39|    4|\n",
      "| 40|    5|\n",
      "| 41|    7|\n",
      "| 42|    4|\n",
      "| 43|    4|\n",
      "| 44|   10|\n",
      "| 45|    6|\n",
      "| 46|    5|\n",
      "| 47|    2|\n",
      "| 48|    6|\n",
      "| 49|    2|\n",
      "| 50|    8|\n",
      "| 51|    3|\n",
      "| 52|    8|\n",
      "| 53|    5|\n",
      "| 54|    4|\n",
      "| 55|    6|\n",
      "| 56|    8|\n",
      "| 57|    1|\n",
      "| 58|    9|\n",
      "| 59|    3|\n",
      "| 60|    5|\n",
      "| 61|    6|\n",
      "| 62|    6|\n",
      "| 63|    1|\n",
      "| 64|    6|\n",
      "| 65|    1|\n",
      "| 66|    7|\n",
      "| 67|    6|\n",
      "| 68|    7|\n",
      "| 69|    4|\n",
      "| 70|    1|\n",
      "| 71|    7|\n",
      "| 72|    9|\n",
      "| 73|    9|\n",
      "| 74|    4|\n",
      "| 75|    2|\n",
      "| 76|    2|\n",
      "| 77|    5|\n",
      "| 78|    3|\n",
      "| 79|    6|\n",
      "| 80|    9|\n",
      "| 81|   10|\n",
      "| 82|    9|\n",
      "| 83|    4|\n",
      "| 84|    2|\n",
      "| 85|    6|\n",
      "| 86|    9|\n",
      "| 87|    9|\n",
      "| 88|    6|\n",
      "| 89|    9|\n",
      "| 90|    2|\n",
      "| 91|    2|\n",
      "| 92|    4|\n",
      "| 93|    9|\n",
      "| 94|    9|\n",
      "| 95|    8|\n",
      "| 96|    3|\n",
      "| 97|    9|\n",
      "| 98|    8|\n",
      "| 99|    5|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setting random number seed\n",
    "seed_no = 42\n",
    "\n",
    "# Creating spark dataframe\n",
    "df = spark.range(100)\n",
    "df = df.union(df)\n",
    "df = df.withColumn(\"value\", F.ceil(F.rand(seed_no) * 10))\n",
    "df.show(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```R\n",
    "seed_no <- 42L\n",
    "df = sparklyr::sdf_seq(sc,from = 1, to = 100)\n",
    "df <- sdf_bind_rows(df,df)\n",
    "df <- df %>% sparklyr::mutate(value = ceil(rand(seed_no) * -10)) %>%\n",
    "        sparklyr::mutate(id = double(id))\n",
    "df %>% head(5) %>% print()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `.crosstab()` function, we can calculate a pair-wise frequency table of the `id` and `value` columns (a.k.a. contingency table). We will generate this table and convert it to a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_value</th>\n",
       "      <th>-1</th>\n",
       "      <th>-2</th>\n",
       "      <th>-3</th>\n",
       "      <th>-4</th>\n",
       "      <th>-5</th>\n",
       "      <th>-6</th>\n",
       "      <th>-7</th>\n",
       "      <th>-8</th>\n",
       "      <th>-9</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id_value  -1  -2  -3  -4  -5  -6  -7  -8  -9  0\n",
       "0        7   0   0   0   0   0   0   0   1   0  1\n",
       "1       51   0   1   0   0   0   1   0   0   0  0\n",
       "2       15   0   0   0   1   0   0   1   0   0  0\n",
       "3       54   0   0   1   0   0   1   0   0   0  0\n",
       "4       11   0   0   0   0   0   1   0   0   1  0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_spark = df.crosstab('id','value')\n",
    "freq_pandas = freq_spark.toPandas()\n",
    "freq_pandas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have brought our pair-wise frequency table into our local environment, we can now utilise the `scipy.stats.chi2_contingency()` function. From the [documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html), we can see that this takes an `array_like` input, as such we should convert our table into a Numpy array:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```R\n",
    "freq_spark <- sdf_crosstab(df, 'id', 'value')\n",
    "freq_spark %>% head(5) %>% print()\n",
    "\n",
    "freq_r <- freq_spark %>% collect() \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_numpy = np.array(freq_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example python code for this has been adapted from the article linked previously. We have opted to restructured this into a function, so it can be ran a few times. After defining the function, we will attempt to run this on our Numpy array `freq_numpy`. We have implemented a `try`-`except` statement to capture the errors and not stop the full script from running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cramer_v(freq_numpy):\n",
    "    #Chi-squared test statistic, sample size, and minimum of rows and columns\n",
    "    X2 = stats.chi2_contingency(freq_numpy, correction=False)[0]\n",
    "    n = np.sum(freq_numpy)\n",
    "    minDim = min(freq_numpy.shape)-1\n",
    "\n",
    "    #calculate Cramer's V \n",
    "    V = np.sqrt((X2/n) / minDim)\n",
    "    return V\n",
    "\n",
    "try:\n",
    "    get_cramer_v(freq_numpy)\n",
    "except TypeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `TypeError` says we have strings where we need integers. We should return to our spark pair-wise frequency table to double check the data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_value: string (nullable = false)\n",
      " |-- -1: long (nullable = false)\n",
      " |-- -2: long (nullable = false)\n",
      " |-- -3: long (nullable = false)\n",
      " |-- -4: long (nullable = false)\n",
      " |-- -5: long (nullable = false)\n",
      " |-- -6: long (nullable = false)\n",
      " |-- -7: long (nullable = false)\n",
      " |-- -8: long (nullable = false)\n",
      " |-- -9: long (nullable = false)\n",
      " |-- 0: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "freq_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the `id_value` column is a type string. So the first variable passed to `.crosstab()` comes out as string. Let's fix that using the `.cast` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_value: integer (nullable = true)\n",
      " |-- -1: long (nullable = false)\n",
      " |-- -2: long (nullable = false)\n",
      " |-- -3: long (nullable = false)\n",
      " |-- -4: long (nullable = false)\n",
      " |-- -5: long (nullable = false)\n",
      " |-- -6: long (nullable = false)\n",
      " |-- -7: long (nullable = false)\n",
      " |-- -8: long (nullable = false)\n",
      " |-- -9: long (nullable = false)\n",
      " |-- 0: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "freq_spark = freq_spark.withColumn(\"id_value\", F.col(\"id_value\").cast(IntegerType()))\n",
    "freq_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have set `id_value` is `int`, we can carry on with rest of processing again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1935164295375403\n"
     ]
    }
   ],
   "source": [
    "freq_pandas = freq_spark.toPandas()\n",
    "freq_numpy = np.array(freq_pandas)\n",
    "print(get_cramer_v(freq_numpy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1928.603337799327 5150 10\n",
      "(100, 11)\n"
     ]
    }
   ],
   "source": [
    "X2 = stats.chi2_contingency(freq_numpy, correction=False)[0]\n",
    "n = np.sum(freq_numpy)\n",
    "minDim = min(freq_numpy.shape)-1\n",
    "print(X2,n,minDim)\n",
    "print((freq_spark.count(), len(freq_spark.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we get the result of about 0.19, telling us that there is a weak association between `id` and `value`.\n",
    "\n",
    "#### Errors with negative values\n",
    "We also note that all frequencies must be positive. If there is a negative value in frequency table, we will get an error. \n",
    "\n",
    "To demonstrate, let's plant some negative values in `freq_numpy`, then calculate the statistic again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_numpy[0][1] = -1\n",
    "freq_numpy[10, 4] = -5\n",
    "\n",
    "try:\n",
    "    get_cramer_v(freq_numpy)\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `ValueError` tells us we have negative values. Suppose we did not know the amount or where these negative values were located. We can index Numpy arrays in a similar way to Pandas DataFrames to see how many and where the negative values are located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many negative values\n",
    "print(' Number of negative values:', len(freq_numpy[freq_numpy < 0]))\n",
    "\n",
    "# Where are they located?\n",
    "rows, cols = np.where(freq_numpy < 0)\n",
    "print(' row of negative value(s):', rows,\n",
    "      '\\n column of negative value(s):', cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is two negative values (shock!) and they are located at [0,1] and [10,4]. Again we use index our numpy array to extract the values at these two points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_numpy[rows,cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further resources\n",
    "\n",
    "Spark at the ONS Articles:\n",
    "\n",
    "\n",
    "PySpark Documentation:\n",
    "- [`.rand()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.rand.html)\n",
    "- [`.ceil()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.ceil.html)\n",
    "- [`.crosstab()`](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrame.crosstab.html)\n",
    "\n",
    "Python Documentation:\n",
    "- [`subprocess`](https://docs.python.org/3/library/subprocess.html) \n",
    "\n",
    "sparklyr and tidyverse Documentation:\n",
    "- [`sdf_coalesce()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_coalesce.html)\n",
    "- [`sdf_repartition()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_repartition.html)\n",
    "- [`sdf_num_partitions()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_num_partitions.html)\n",
    "- [`spark_apply()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/spark_apply.html)\n",
    "\n",
    "Spark Documentation:\n",
    "- [Spark Configuration](https://spark.apache.org/docs/latest/configuration.html):\n",
    "    - [Execution Behaviour](https://spark.apache.org/docs/latest/configuration.html#execution-behavior)\n",
    "    - [Runtime SQL Configuration](https://spark.apache.org/docs/latest/configuration.html#runtime-sql-configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1928.603337799327 5150 10\n",
      "[[0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]\n",
      " [0.19 0.13 0.15 0.21 0.15 0.16 0.2  0.22 0.27 0.32]]\n",
      "0.6761929030278736\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(100)\n",
    "df = df.union(df)\n",
    "df = df.withColumn(\"value\", F.ceil(F.rand(seed_no) * 10))\n",
    "freq_spark = df.crosstab('id','value')\n",
    "freq_spark = freq_spark.withColumn(\"id_value\", F.col(\"id_value\").cast(IntegerType()))\n",
    "freq_pandas = freq_spark.toPandas()\n",
    "freq_numpy = np.array(freq_pandas)\n",
    "\n",
    "X2 = stats.chi2_contingency(freq_numpy, correction=False)[0]\n",
    "n = np.sum(freq_numpy)\n",
    "minDim = min(freq_numpy.shape)-1\n",
    "print(X2,n,minDim)\n",
    "freq_pandas_index = freq_pandas.set_index('id_value')\n",
    "freq_numpy_index = np.array(freq_pandas_index)\n",
    "print(stats.chi2_contingency(freq_numpy_index, correction=False)[3])\n",
    "print(get_cramer_v(freq_numpy_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 1, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 2],\n",
       "       [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 1, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 0, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "       [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 1],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "       [0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 2, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "       [0, 1, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 2, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 1, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pd = df.toPandas()\n",
    "import pandas as pd\n",
    "freq = pd.crosstab(df_pd['id'], df_pd['value'])\n",
    "np.array(freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16174359558286786\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9417956656346751,\n",
       " 0.33181646987425817,\n",
       " 1,\n",
       " array([[ 8.44444444, 10.55555556],\n",
       "        [ 7.55555556,  9.44444444]]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load necessary packages and functions\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "#create 2x2 table\n",
    "data = np.array([[7,12], [9,8]])\n",
    "\n",
    "#Chi-squared test statistic, sample size, and minimum of rows and columns\n",
    "X2 = stats.chi2_contingency(data, correction=False)[0]\n",
    "n = np.sum(data)\n",
    "minDim = min(data.shape)-1\n",
    "\n",
    "#calculate Cramer's V \n",
    "V = np.sqrt((X2/n) / minDim)\n",
    "\n",
    "#display Cramer's V\n",
    "print(V)\n",
    "stats.chi2_contingency(data, correction=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open(\"../../../config.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "# rescue_path = config[\"rescue_path_csv\"]\n",
    "rescue_path = \"../../data/animal_rescue.csv\"\n",
    "rescue = spark.read.csv(rescue_path, header=True, inferSchema=True)\n",
    "rescue = rescue.withColumnRenamed('AnimalGroupParent','animal_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "452.2733832747976 5898 10\n",
      "[[-7.35164463e+00  2.04815192e-01  1.02407596e-01  1.90369617e+00\n",
      "  -2.83146829e-01 -4.37368600e+00 -3.77314344e+00  5.12037979e-01\n",
      "   2.04815192e-01 -4.62699220e+00  1.02407596e-01 -2.56629366e+00\n",
      "   1.02407596e-01  4.28280773e+00  2.04815192e-01  3.07222787e-01\n",
      "   4.09630383e-01 -9.75924042e-01 -5.90369617e-01 -1.80739234e-01\n",
      "   1.60834181e+00  1.02407596e-01  3.07222787e-01  8.59003052e+00\n",
      "   4.60834181e+00  6.32417769e-01  5.36113937e-01]\n",
      " [ 4.61851475e-01  2.04476094e-01  1.02238047e-01 -4.58952187e+00\n",
      "  -2.28433367e+00  2.61037640e+00  3.05595117e+00 -4.88809766e-01\n",
      "   2.04476094e-01  1.03326551e+01  1.02238047e-01  1.43133266e+00\n",
      "   1.02238047e-01 -1.17441506e+01 -7.95523906e-01  3.06714140e-01\n",
      "   4.08952187e-01  1.02238047e+00 -5.91047813e-01  8.17904374e-01\n",
      "   6.00712106e-01  1.02238047e-01 -6.93285860e-01 -1.43743642e+00\n",
      "   6.00712106e-01  1.62309257e+00 -1.46642930e+00]\n",
      " [-2.28585961e+00  1.22075280e-02  6.10376399e-03  1.75584944e+00\n",
      "   4.27263479e-02 -1.42624619e+00  5.15259410e+00  3.05188199e-02\n",
      "   1.22075280e-02 -5.47304171e-01  6.10376399e-03  8.54526958e-02\n",
      "   6.10376399e-03 -2.95015259e-02  1.22075280e-02  1.83112920e-02\n",
      "   2.44150560e-02  6.10376399e-02  2.44150560e-02  4.88301119e-02\n",
      "  -7.25330621e-01  6.10376399e-03 -9.81688708e-01 -2.01119023e+00\n",
      "   2.74669379e-01  3.35707019e-01  9.15564598e-02]\n",
      " [-2.34743981e+01  1.82773822e-01  9.13869108e-02  8.84452357e+00\n",
      "   6.39708376e-01 -2.40963038e+00  1.11180061e+01  4.56934554e-01\n",
      "   1.82773822e-01 -1.12499152e+01  9.13869108e-02  1.27941675e+00\n",
      "   9.13869108e-02  4.53051882e+00  1.82773822e-01  2.74160732e-01\n",
      "  -6.34452357e-01  9.13869108e-01  3.65547643e-01 -2.68904713e-01\n",
      "  -8.87589013e-01  9.13869108e-02  2.74160732e-01  6.80467955e+00\n",
      "   4.11241099e+00 -1.97371991e+00  3.70803662e-01]\n",
      " [-1.26822652e+00  1.97694134e-01  9.88470668e-02 -7.45388267e+00\n",
      "  -3.08070532e-01  4.29162428e+00  9.63784334e+00  4.94235334e-01\n",
      "   1.97694134e-01  1.52560190e+00 -9.01152933e-01  3.83858935e-01\n",
      "   9.88470668e-02  3.71668362e+00  1.97694134e-01  2.96541200e-01\n",
      "  -6.04611733e-01 -2.01152933e+00  3.95388267e-01  7.90776534e-01\n",
      "   2.44811801e+00  9.88470668e-02 -7.03458800e-01 -1.29867752e+01\n",
      "   4.44811801e+00 -1.56341133e+00 -1.51729400e+00]\n",
      " [ 2.41047813e+01  1.98372330e-01  9.91861648e-02 -2.34674466e+01\n",
      "   6.94303154e-01  2.32349949e+00  6.97965412e+00  4.95930824e-01\n",
      "   1.98372330e-01 -1.39369278e+00  9.91861648e-02 -1.61139369e+00\n",
      "   9.91861648e-02 -2.29399797e-01  1.98372330e-01  2.97558494e-01\n",
      "  -6.03255341e-01  9.91861648e-01  3.96744659e-01 -1.20651068e+00\n",
      "  -2.53662258e+00  9.91861648e-02  2.97558494e-01 -2.93184130e+00\n",
      "   2.46337742e+00 -6.54476094e+00  4.87792472e-01]\n",
      " [-1.24191251e+01 -7.93489318e-01  1.03255341e-01 -3.63021363e+00\n",
      "   7.22787386e-01 -6.29399797e+00  1.30813835e+01 -4.83723296e-01\n",
      "   2.06510682e-01 -8.42522889e+00  1.03255341e-01  1.44557477e+00\n",
      "   1.03255341e-01  4.41759919e+00  2.06510682e-01 -6.90233978e-01\n",
      "  -5.86978637e-01 -9.67446592e-01  4.13021363e-01 -1.73957274e-01\n",
      "  -3.53509664e-01  1.03255341e-01  3.09766022e-01  9.72736521e+00\n",
      "   2.64649034e+00  6.79043744e-01  5.48830112e-01]\n",
      " [ 1.69342150e+01  1.92607664e-01  9.63038318e-02  1.81478467e+01\n",
      "   6.74126823e-01  1.05256019e+00 -3.49257375e+01  4.81519159e-01\n",
      "   1.92607664e-01  6.92031197e+00  9.63038318e-02  1.34825365e+00\n",
      "  -9.03696168e-01 -3.68769074e+00  1.92607664e-01 -7.11088505e-01\n",
      "   3.85215327e-01 -3.69616819e-02 -6.14784673e-01 -2.22956935e+00\n",
      "   3.33672431e-01  9.63038318e-02  2.88911495e-01  5.60122075e+00\n",
      "  -9.66632757e+00 -7.03289251e-01  4.44557477e-01]\n",
      " [-4.36758223e+00 -7.89759240e-01  1.05120380e-01 -3.20481519e+00\n",
      "   7.35842659e-01  8.81315700e-01  2.96134283e+00 -1.47439810e+00\n",
      "   2.10240760e-01 -9.81349610e-01  1.05120380e-01 -1.52831468e+00\n",
      "   1.05120380e-01 -5.28585961e+00  2.10240760e-01  3.15361139e-01\n",
      "   4.20481519e-01  1.05120380e+00  4.20481519e-01  8.40963038e-01\n",
      "   7.30417091e-01  1.05120380e-01  3.15361139e-01  4.02950153e+00\n",
      "  -3.26958291e+00  5.78162089e+00  1.57680570e+00]\n",
      " [-5.28789420e+00  1.83112920e-01  9.15564598e-02  4.33774161e+00\n",
      "   6.40895219e-01  2.60630722e+00  4.28891150e+00 -5.42217701e-01\n",
      "   1.83112920e-01  7.90437436e-01  9.15564598e-02  1.28179044e+00\n",
      "   9.15564598e-02  2.55747711e+00  1.83112920e-01  2.74669379e-01\n",
      "   3.66225839e-01  9.15564598e-01 -6.33774161e-01  7.32451679e-01\n",
      "  -8.79959308e-01 -9.08443540e-01  2.74669379e-01 -1.41678535e+01\n",
      "   4.12004069e+00 -1.96439471e+00  3.73346897e-01]\n",
      " [ 1.49538827e+01  2.07188878e-01 -8.96405561e-01  7.35622245e+00\n",
      "  -1.27483893e+00  7.37877247e-01 -1.75768057e+01  5.17972194e-01\n",
      "  -1.79281112e+00  7.65547643e+00  1.03594439e-01 -1.54967786e+00\n",
      "   1.03594439e-01  1.47151577e+00 -7.92811122e-01 -6.89216684e-01\n",
      "   4.14377755e-01 -9.64055612e-01  4.14377755e-01  8.28755510e-01\n",
      "  -3.38250254e-01  1.03594439e-01  3.10783316e-01 -1.21770092e+00\n",
      "  -1.03382503e+01  3.69769413e+00 -1.44608342e+00]]\n"
     ]
    }
   ],
   "source": [
    "rescue_crosstab = rescue.crosstab('CalYear','animal_type')\n",
    "\n",
    "rescue_numpy = np.array(rescue_crosstab.toPandas().set_index('CalYear_animal_type'))\n",
    "get_cramer_v(rescue_numpy)\n",
    "X2 = stats.chi2_contingency(rescue_numpy, correction=False)[0]\n",
    "n = np.sum(rescue_numpy)\n",
    "minDim = min(rescue_numpy.shape)-1\n",
    "print(X2,n,minDim)\n",
    "expected = stats.chi2_contingency(rescue_numpy, correction=False)[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+------+----+---+---+----+---+------+----+---+----+-------+--------+-----+----+------+------+------+-----+-----+--------+--------+------------------------------------------------+--------------------------------+--------------------------------+---------------------+---+\n",
      "|CalYear_animal_type|Bird|Budgie|Bull|Cat|Cow|Deer|Dog|Ferret|Fish|Fox|Goat|Hamster|Hedgehog|Horse|Lamb|Lizard|Pigeon|Rabbit|Sheep|Snake|Squirrel|Tortoise|Unknown - Animal rescue from water - Farm animal|Unknown - Domestic Animal Or Pet|Unknown - Heavy Livestock Animal|Unknown - Wild Animal|cat|\n",
      "+-------------------+----+------+----+---+---+----+---+------+----+---+----+-------+--------+-----+----+------+------+------+-----+-----+--------+--------+------------------------------------------------+--------------------------------+--------------------------------+---------------------+---+\n",
      "|               2016| 120|     0|   0|296|  1|  14|107|     0|   0| 29|   0|      4|       0|   12|   0|     0|     0|     2|    1|    1|       3|       0|                                               0|                               8|                               0|                    5|  1|\n",
      "|               2012| 112|     0|   0|302|  3|   7|100|     1|   0| 14|   0|      0|       0|   28|   1|     0|     0|     0|    1|    0|       4|       0|                                               1|                              18|                               4|                    4|  3|\n",
      "|               2019|   9|     0|   0| 16|  0|   2|  1|     0|   0|  2|   0|      0|       0|    1|   0|     0|     0|     0|    0|    0|       1|       0|                                               1|                               3|                               0|                    0|  0|\n",
      "|               2017| 124|     0|   0|257|  0|  11| 81|     0|   0| 33|   0|      0|       0|   10|   0|     0|     1|     0|    0|    1|       5|       0|                                               0|                               8|                               0|                    7|  1|\n",
      "|               2014| 110|     0|   0|295|  1|   5| 90|     0|   0| 22|   1|      1|       0|   12|   0|     0|     1|     3|    0|    0|       2|       0|                                               1|                              29|                               0|                    7|  3|\n",
      "|               2013|  85|     0|   0|312|  0|   7| 93|     0|   0| 25|   0|      3|       0|   16|   0|     0|     1|     0|    0|    2|       7|       0|                                               0|                              19|                               2|                   12|  1|\n",
      "|               2018| 126|     1|   0|304|  0|  16| 91|     1|   0| 33|   0|      0|       0|   12|   0|     1|     1|     2|    0|    1|       5|       0|                                               0|                               7|                               2|                    5|  1|\n",
      "|               2009|  89|     0|   0|262|  0|   8|132|     0|   0| 16|   0|      0|       1|   19|   0|     1|     0|     1|    1|    3|       4|       0|                                               0|                              10|                              14|                    6|  1|\n",
      "|               2011| 120|     1|   0|309|  0|   9|103|     2|   0| 26|   0|      3|       0|   22|   0|     0|     0|     0|    0|    0|       4|       0|                                               0|                              13|                               8|                    0|  0|\n",
      "|               2015| 106|     0|   0|262|  0|   6| 88|     1|   0| 21|   0|      0|       0|   12|   0|     0|     0|     0|    1|    0|       5|       1|                                               0|                              29|                               0|                    7|  1|\n",
      "|               2010|  99|     0|   1|294|  2|   9|122|     0|   2| 17|   0|      3|       0|   15|   1|     1|     0|     2|    0|    0|       5|       0|                                               0|                              18|                              15|                    2|  3|\n",
      "+-------------------+----+------+----+---+---+----+---+------+----+---+----+-------+--------+-----+----+------+------+------+-----+-----+--------+--------+------------------------------------------------+--------------------------------+--------------------------------+---------------------+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[IncidentNumber: string, DateTimeOfCall: string, CalYear: int, FinYear: string, TypeOfIncident: string, PumpCount: double, PumpHoursTotal: double, HourlyNotionalCost(£): int, IncidentNotionalCost(£): double, FinalDescription: string, animal_type: string, OriginofCall: string, PropertyType: string, PropertyCategory: string, SpecialServiceTypeCategory: string, SpecialServiceType: string, WardCode: string, Ward: string, BoroughCode: string, Borough: string, StnGroundName: string, PostcodeDistrict: string, Easting_m: double, Northing_m: double, Easting_rounded: int, Northing_rounded: int]>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rescue_crosstab.show()\n",
    "rescue.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28791093176153515"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rescue_crosstab = rescue.crosstab('PostcodeDistrict','animal_type')\n",
    "\n",
    "rescue_numpy = np.array(rescue_crosstab.toPandas().set_index('PostcodeDistrict_animal_type'))\n",
    "get_cramer_v(rescue_numpy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
