---
title: "Logistic Regression"
output: html_document

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Logistic Regression in sparklyr 

- Outline of this in sparklyr
- Spark ML
- predictive vs inferential
- Adapt from [Spark ML overview](https://spark.rstudio.com/guides/mlib.html)?

### Predictive analysis

This section shows how to use the `Spark ML` function `ml_generalised_linear_regression` in `sparklyr` to generate a logistic regression model. The syntax used to call these functions is somewhat similar to logistic regression functions in R. However, there are some additional steps to take in preparing the data before the model can be run successfully. 

#### Preparing the data
We will read the data in as follows:

```{r, eval = FALSE}
sc <- sparklyr::spark_connect(
  master = "local[2]",
  app_name = "sparklyr-lab2",
  config = sparklyr::spark_config())


sparklyr::spark_connection_is_open(sc)

rescue_path_parquet = "/training/rescue_clean.parquet"
rescue <- sparklyr::spark_read_parquet(sc, rescue_path_parquet)
dplyr::glimpse(rescue)
```

Let's see if we can use the `rescue` data to predict whether an animal is a cat or not.

```{r, eval = FALSE}
# Create is_cat column to contain target variable and select relevant predictors
rescue_cat <- rescue %>% 
  dplyr::mutate(is_cat = ifelse(animal_group == "Cat", 1, 0)) %>% 
  sparklyr::select(typeofincident, 
                   engine_count, 
                   job_hours, 
                   hourly_cost, 
                   total_cost, 
                   originofcall, 
                   propertycategory,
                   specialservicetypecategory,
                   specialservicetype,
                   incident_duration,
                   is_cat)

dplyr::glimpse(rescue_cat)
```
Examining the dataset, there are a few columns we have selected above that are of the type "character" when they should be numeric. We can convert all of these to numeric values by running the following:

```{r, eval = FALSE}
rescue_cat <- rescue_cat %>% 
  dplyr::mutate(across(c(engine_count:total_cost, incident_duration), 
                ~as.numeric(.)))

dplyr::glimpse(rescue_cat)
```

#### Missing values

You may already be familiar with functions such as `glm` for performing logistic regression in R. Many of these functions are quite user-friendly and will automatically account for issues such as missing values in the data. However, the `Spark ML` functions in `sparklyr`require the user to correct for these issues before running the regression functions.

```{r, eval = FALSE}

# Get the number of NAs in the dataset by column
rescue_cat %>%
  dplyr::summarise_all(~sum(as.integer(is.na(.)))) %>% 
  print(width = Inf)

# There are 38 missing values in the 4 of the columns 
# We can see that these are all on the same rows:  
rescue_cat %>%
  sparklyr::filter(is.na(total_cost)) %>%
  print(n=38)

# For simplicity, we will just filter out these rows:
rescue_cat <- rescue_cat %>%
    sparklyr::filter(!is.na(total_cost)) 
  
# Double check we have no NAs left 
rescue_cat %>%
  dplyr::summarise_all(~sum(as.integer(is.na(.)))) %>% 
  print(width = Inf)

```
*add something about imputation?*

#### Possible re-structure?

Show running model, then show how to get coefficients, then show how to choose the reference variables?

Makes more sense this way?

Maybe then don't split by inferential and predictive?

#### Selecting reference categories and encoding categorical variables

When including categorical variables as independent variables in a regression, one of the categories must act as the reference category. The coefficients for all other categories are relative to the reference category. 

By default, the `ml_generalized_linear_regression` function in R will select the least common category as the reference category before running the model, without input from the user. It also implicitly applies one-hot encoding to the categories so that they can be represented as a binary numerical value. 

For example, `specialservicetypecategory` has four unique values: Animal Rescue From Below Ground, Animal Rescue From Height, Animal Rescue From Water, and Other Animal Assistance. 

```{r, eval = FALSE}
rescue_cat %>% 
  dplyr::count(specialservicetypecategory) %>% 
  dplyr::arrange(n)
```

In this case, `ml_generalized_linear_regression` would select "Animal Rescue From Water" as the reference category, since there are only 343 instances of it in the data. For the purpose of running the model, it would also create a new column in the data for each of the other categories, assigning a 1 or 0 for each row depending on whether that entry belongs to that category or not. Regression coefficients would then be shown for each of these new columns in the final model, relative to the to the "Animal Rescue From Water" reference category. 

If we wanted to specify which category we want to choose as our reference category in R, this could be done quite simply using **factors**. For example, to choose "Animal Rescue From Height" as our reference category instead, we would just need to mutate the `specialservicetypecategory` as a factor and specify the reference category as follows:

```{r, eval = FALSE}
rescue_cat <- rescue_cat %>% 
  mutate(specialservicetypecategory_ref = relevel(as.factor(specialservicetypecategory), 
                  ref = "Animal Rescue From Height"))
```

This also makes it very easy to specify reference categories for multiple columns at once by adding in additional mutate terms.

Unfortunately, factors do not exist in Spark and therefore are not a valid data type in sparklyr. So we need to find an alternative way of specifying reference categories.

Instead, we can make use of the one-hot encoding concept and two of `sparklyr`'s **Feature Transformers** (`ft_string_indexer` and `ft_one_hot_encoder`) to achieve this. These functions have limited functionality, so we must first manipulate it such that the reference category will be ordered last when using `ft_string_indexer`, and then we can drop the last category using `ft_one_hot_encoder`. A convenient way of doing this is to order the categories in *descending alphabetical* order and ensuring that our chosen category will be ordered last by adding an appropriate prefix to it. For example, adding `000_`:

```{r, eval = FALSE}
rescue_cat_ohe <- rescue_cat %>%
  mutate(region = ifelse(specialservicetypecategory == "Animal Rescue From Height",
                         "000_Animal Rescue From Height",
                         specialservicetypecategory)) %>%
  ft_string_indexer(input_col = "specialservicetypecategory", 
                    output_col = "specialservicetypecategory_idx",
                    string_order_type = "alphabetDesc") %>%
  ft_one_hot_encoder(input_cols = c("specialservicetypecategory_idx"), 
                     output_cols = c("specialservicetypecategory_ohe"), 
                     drop_last = TRUE) %>%
  sdf_separate_column(column = "region_ohe",
                      into = c("region_Wales", "region_Yorkshire_and_the_Humber", "region_West_Midlands", "region_South_West", "region_South_East", "region_North_West", "region_North_East", "region_East_of_England", "region_East_Midlands"))
```




**Need to add something about dependent variables - use specialservicetype and specialservicetypecategory as an example**

#### Generating a model



### Inferential analysis
**Adapt from Ted's presentation materials**

The `Spark ML` functions available in sparklyr were largely developed with predictive analysis in mind. This means that they have been built primarily to specify a regression model and retrieve the prediction (as we have seen above), with little information in between.

When conducting analysis at ONS, we are often not interested in predicting unknown outcomes, but instead understanding the relationship between the independent variables and the probability of the outcome. This is what is referred to as *inferential analysis* in this section.

The guide below shows how to find regression coefficients, test significance, calculate confidence intervals and rebase categorical variables to produce regression output tables for inferential purposes.




