{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Spark Job Description\n",
    "\n",
    "```{warning}\n",
    "This section is under construction.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spark UI is a great tool for: monitoring Spark applications; troubleshooting slow jobs; and generally understanding how spark will execute your code. However, it is sometimes difficult to find the correct job number to drill down on a problem. \n",
    "\n",
    "There is a way to set the Spark job description when using Pyspark, making use of the `spark.sparkContext.setJobDescription()` function. This function takes a string input and will update the description column in the spark UI with this string. While this makes finding the job in the UI much easier, if we do not update the descriptions in our script or tell spark to [revert back to default descriptions](#back-to-default-description) each job will be given the same description. \n",
    "\n",
    "We will work through a short example to highlight its effectiveness. First we will start a Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"job-description-tip\")\n",
    "    .config(\"spark.executor.memory\", \"1g\")\n",
    "    .config(\"spark.executor.cores\", 1)\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", 3)\n",
    "    .config(\"spark.sql.shuffle.partitions\", 12)\n",
    "    .config(\"spark.shuffle.service.enabled\", \"true\")\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"false\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll generate a link to the Spark UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, IPython\n",
    "url = \"spark-%s.%s\" % (os.environ[\"CDSW_ENGINE_ID\"], os.environ[\"CDSW_DOMAIN\"])\n",
    "IPython.display.HTML(\"<a href=http://%s>Spark UI</a>\" % url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default descriptions\n",
    "By default spark will generate job descriptions based on the action that has been called.\n",
    "Now we will read in some data to perform some transformations and some actions to create a few Spark jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescue = (spark.read.csv(\"/training/animal_rescue.csv\", header=True) \n",
    "      .withColumnRenamed(\"AnimalGroupParent\", \"AnimalGroup\") \n",
    "      .select(\"IncidentNumber\", \"FinalDescription\", \"AnimalGroup\", \"CalYear\")\n",
    "     )\n",
    "rescue.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescue.sort(\"CalYear\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescue.select(\"IncidentNumber\", \"AnimalGroup\").sort(\"IncidentNumber\").limit(3).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check the Spark UI, there's screenshot included below. As we can see, a number of jobs have been created, but which action created each job?\n",
    "\n",
    "![Spark UI, jobs tab showing default descriptions for the jobs](path_to_files)\n",
    "\n",
    "In the *Description* column we can see the description starts with the action that created the job. This is useful to identify the correct job, but we can do one better.\n",
    "\n",
    "## Customised description\n",
    "\n",
    "We can customise the job description using `spark.sparkContext.setJobDescription()`. This is useful as we can assign a more detailed name to each job within our code. Doing so will help us understand and find the exact action which has created a job in the spark UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setJobDescription(\"Count fox incidents\")\n",
    "\n",
    "rescue.filter(F.col(\"AnimalGroup\") == \"Fox\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setJobDescription(\"Count incidents in 2015\")\n",
    "\n",
    "rescue.filter(F.col(\"CalYear\") == 2015).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the UI again you can see our descriptions have been updated to contain our customised names.\n",
    "\n",
    "![Spark UI, jobs tab showing default descriptions and customised descriptions for the jobs](Update_path)\n",
    "\n",
    "**WARNING** every job from now on will have the last description we set, unless we tell spark to default back to the default descriptions.\n",
    "\n",
    "## Back to default description\n",
    "\n",
    "We can set the description to `None` to revert back to the default descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setJobDescription(None)\n",
    "\n",
    "rescue.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job description for the most recent job has been revered to its default:\n",
    "\n",
    "![Spark UI, jobs tab showing default and customised descriptions for the jobs](Update_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Default job descriptions tell you the action used to trigger the job\n",
    "- Set a job description to better track jobs in the Spark UI using `spark.sparkContext.setJobDescription()` \n",
    "- Remember to set the description to `None` to go back to using default descriptions once you've finished tracking your jobs. \n",
    "- The description will carry through to the Stages tab also, but will not appear in the SQL tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Resources\n",
    "\n",
    "Spark at the ONS Articles:\n",
    "- [Spark Application and UI](../spark-concepts/spark-application-and-ui)\n",
    "\n",
    "PySpark Documentation:\n",
    "- [`SparkSession`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.html)\n",
    "- [`.count()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.count.html)\n",
    "- [`.groupBy()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.groupBy.html)\n",
    "- [`.show()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.show.html)\n",
    "- [`setJobDescription`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.setJobDescription.html?highlight=setjob)\n",
    "\n",
    "Spark documentation:\n",
    "- [Monitoring and Instrumentation](https://spark.apache.org/docs/latest/monitoring.html)\n",
    "- [Spark Web UI](https://spark.apache.org/docs/latest/web-ui.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Resources\n",
    "\n",
    "Spark at the ONS Articles:\n",
    "- [Partitions](../spark-concepts/partitions)\n",
    "- [Shuffling](../spark-concepts/shuffling)\n",
    "- [Persisting](../spark-concepts/persistence)\n",
    "- [Optimising Joins](../spark-concepts/join-concepts)\n",
    "- [Garbage Collection](../spark-concepts/garbage-collection)\n",
    "- [Set Spark Job Description](../spark-functions/job-description) \n",
    "- [Spark Application and UI](../spark-concepts/spark-application-and-ui.md)\n",
    "\n",
    "PySpark Documentation:\n",
    "- [`SparkSession`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.html)\n",
    "- [`.count()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.count.html)\n",
    "- [`.groupBy()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.groupBy.html)\n",
    "- [`.join()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.join.html)\n",
    "- [`spark.stop()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.stop.html)\n",
    "- [`.show()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.show.html)\n",
    "- [`.toPandas()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.toPandas.html)\n",
    "- [`.agg()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.agg.html)\n",
    "- [`.coalesce()`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.coalesce.html)\n",
    "\n",
    "sparklyr and tidyverse Documentation:\n",
    "- [`spark_connect()` and `spark_disconnect()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/spark-connections.html)\n",
    "- [`group_by()`](https://dplyr.tidyverse.org/reference/group_by.html)\n",
    "- [`left_join()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/join.tbl_spark.html)\n",
    "- [`sdf_nrow()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_dim.html)\n",
    "- [`collect()`](https://dplyr.tidyverse.org/reference/compute.html)\n",
    "- [`summarise()`](https://dplyr.tidyverse.org/reference/summarise.html)\n",
    "- [`sdf_coalesce()`](https://spark.rstudio.com/packages/sparklyr/latest/reference/sdf_coalesce.html)\n",
    "\n",
    "Spark documentation:\n",
    "- [Monitoring and Instrumentation](https://spark.apache.org/docs/latest/monitoring.html)\n",
    "- [Spark Web UI](https://spark.apache.org/docs/latest/web-ui.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
